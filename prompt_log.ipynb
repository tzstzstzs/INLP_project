{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIAO2AYpoVkUIFCx9y3VHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzstzstzs/INLP_project/blob/master/prompt_log.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k71pDV62Ys-U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the best metrics for evaluating the models of this project?\n",
        "\n",
        "Okay, let's discuss the best metrics for evaluating the models in your specific project, considering the code you've provided.\n",
        "\n",
        "Your Project Context:\n",
        "\n",
        "You're using the Hugging Face Transformers library for sentiment classification on the IMDB dataset.\n",
        "You're training three different models: DistilBERT, BERT, and RoBERTa.\n",
        "You're already calculating eval_loss, eval_accuracy, and eval_runtime.\n",
        "Recommended Metrics:\n",
        "\n",
        "Accuracy: Since sentiment classification is often a balanced binary classification problem (positive/negative), accuracy is a reasonable starting point. It provides a straightforward measure of overall correctness. You're already tracking this in your code.\n",
        "\n",
        "F1-Score: While accuracy is good, F1-score provides a more balanced view, especially if there's a slight class imbalance. It's the harmonic mean of precision and recall and is a good indicator of overall performance.\n",
        "\n",
        "AUC (Area Under the ROC Curve): AUC is particularly useful for binary classification. It measures the model's ability to distinguish between positive and negative sentiments across different probability thresholds. A higher AUC generally indicates better performance.\n",
        "\n",
        "Adding F1-Score and AUC to Your Code:\n",
        "\n",
        "Here's how you can add the calculation of F1-score and AUC to your evaluation process:\n",
        "\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Evaluate the first model\n",
        "eval_result = trainer.evaluate()\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)))\n",
        "predicted_labels = predictions.predictions.argmax(-1) # Get predicted labels\n",
        "true_labels = predictions.label_ids # Get true labels\n",
        "eval_result['eval_f1'] = f1_score(true_labels, predicted_labels)\n",
        "eval_result['eval_auc'] = roc_auc_score(true_labels, predictions.predictions[:, 1]) # Assuming binary classification\n",
        "\n",
        "# ... (similarly for other models) ...\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "results_data = {\n",
        "    'Model': [model_name, model_name2, model_name3],\n",
        "    # ... (other metrics) ...\n",
        "    'eval_f1': [eval_result.get('eval_f1'), eval_result2.get('eval_f1'), eval_result3.get('eval_f1')],\n",
        "    'eval_auc': [eval_result.get('eval_auc'), eval_result2.get('eval_auc'), eval_result3.get('eval_auc')]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "display(results_df)\n",
        "Körültekintően használja a kódot\n",
        "Explanation:\n",
        "\n",
        "We import f1_score and roc_auc_score from sklearn.metrics.\n",
        "After evaluating each model, we use trainer.predict to get predictions on the evaluation set.\n",
        "We extract predicted labels (predicted_labels) and true labels (true_labels).\n",
        "We calculate F1-score and AUC using these labels and add them to the eval_result dictionary.\n",
        "Finally, we include these metrics in the results_df for comparison.\n",
        "Important Considerations:\n",
        "\n",
        "You might need to adjust the code slightly based on the specific structure of your predictions object.\n",
        "Consider using the average parameter in f1_score if you have multi-class classification.\n",
        "Remember that these metrics are just tools to guide your model selection and improvement. Ultimately, the best model is the one that best addresses your specific needs and requirements. I hope this helps! Let me know if you have any other questions.\n",
        "\n",
        "\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "\n",
        "What are the good values of these metrics?\n",
        "\n",
        "Okay, let's discuss what are considered good values for the metrics you're using (accuracy, F1-score, and AUC) in the context of text classification, especially for sentiment analysis on the IMDB dataset.\n",
        "\n",
        "General Guidelines:\n",
        "\n",
        "Accuracy: For balanced datasets, accuracy above 80% is often considered good. However, for imbalanced datasets, accuracy can be misleading, and you should focus on other metrics like F1-score or AUC. In sentiment analysis, accuracy above 85% is generally considered very good.\n",
        "\n",
        "F1-Score: F1-score ranges from 0 to 1, with 1 being the best. Values above 0.8 are often considered good. Similar to accuracy, sentiment analysis models often achieve F1-scores above 0.85.\n",
        "\n",
        "AUC: AUC also ranges from 0 to 1, with 1 being perfect classification. Values above 0.8 are generally considered good. For sentiment analysis tasks, AUC scores above 0.9 are often seen as excellent.\n",
        "\n",
        "Specific to Your Project:\n",
        "\n",
        "Since you're working with the IMDB dataset, which is a relatively balanced dataset, you can consider accuracy to be a useful metric. However, it's always good to look at F1-score and AUC as well for a more complete picture.\n",
        "Based on the state-of-the-art results on IMDB sentiment analysis, you should aim for accuracy and F1-score above 90% and AUC above 0.95. However, remember that these are very high standards, and achieving them might require significant model tuning and experimentation.\n",
        "Interpreting Your Results:\n",
        "\n",
        "Accuracy: If your accuracy is below 80%, it suggests there's room for improvement in your model's overall performance.\n",
        "F1-Score: If your F1-score is significantly lower than your accuracy, it could indicate that your model is struggling with one of the classes (either positive or negative).\n",
        "AUC: If your AUC is below 0.8, it means your model is having difficulty distinguishing between positive and negative sentiments.\n",
        "Factors to Consider:\n",
        "\n",
        "Dataset Size and Quality: The size and quality of your training data can greatly influence the performance of your model. More data and higher quality data generally lead to better results.\n",
        "Model Architecture: Different model architectures (like DistilBERT, BERT, and RoBERTa) have different strengths and weaknesses. Experimenting with different architectures can help you find the best one for your task.\n",
        "Hyperparameter Tuning: Fine-tuning the hyperparameters of your model (e.g., learning rate, batch size) can significantly impact performance.\n",
        "Ultimately, the definition of \"good\" values for these metrics depends on your specific application and the trade-offs between different types of errors. It's important to understand the strengths and limitations of each metric and to consider your overall goals when interpreting your results. I hope this helps! Let me know if you have any other questions.\n",
        "\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "\n",
        "Modify my project so it will save the files of the trained model for later use then reads them when I want to test the models again\n",
        "\n",
        "Okay, I can help you with that. Here's how you can modify your project to save and load your trained models:\n",
        "\n",
        "Saving the trained models\n",
        "\n",
        "After training each model, you can save it using the save_pretrained() method. This will save the model's architecture and weights to a specified directory. Here's an example of how to save the first model:\n",
        "\n",
        "trainer.save_model(\"./my_distilbert_model\")\n",
        "tokenizer.save_pretrained(\"./my_distilbert_model\")\n",
        "Körültekintően használja a kódot\n",
        "This will save the model and tokenizer to a directory named \"my_distilbert_model\". You can change the directory name to whatever you like. You can repeat this for trainer2 and trainer3 with their respective model names. For example:\n",
        "\n",
        "trainer2.save_model(\"./my_bert_model\")\n",
        "tokenizer2.save_pretrained(\"./my_bert_model\")\n",
        "\n",
        "trainer3.save_model(\"./my_roberta_model\")\n",
        "tokenizer3.save_pretrained(\"./my_roberta_model\")\n",
        "Körültekintően használja a kódot\n",
        "Loading the trained models\n",
        "\n",
        "To load the saved models, you can use the from_pretrained() method. Here's an example of how to load the first model:\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_path = \"./my_distilbert_model\"  # Path to saved model\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "Körültekintően használja a kódot\n",
        "This will load the model and tokenizer from the specified directory. You can then use the loaded model to make predictions. To load the other two models, simply change the model_path to their respective directories.\n",
        "\n",
        "Integrating saving and loading in your project\n",
        "\n",
        "Here's how you can integrate the saving and loading process into your existing project:\n",
        "\n",
        "Add the saving code after training: After calling trainer.train(), add the save_model and save_pretrained lines to save the model and tokenizer.\n",
        "Modify the model loading part: Instead of loading models directly from the Hugging Face model hub using from_pretrained with only a model name, use it with the paths to your saved models as shown in the loading example.\n",
        "Reasoning\n",
        "\n",
        "Saving models allows you to reuse them later without retraining, thus saving you time and computational resources. It also enables you to share your models with others. By loading a pre-trained model, you can quickly set up and run experiments without lengthy training.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions.\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "What is the most reasonable way to of adjusting or tuning the models?\n",
        "\n",
        "Okay, let's discuss how to adjust and tune the models you've trained for better performance. Here's a breakdown of reasonable approaches, focusing on hyperparameter optimization:\n",
        "\n",
        "1. Hyperparameter Optimization\n",
        "\n",
        "Hyperparameters are settings that control the learning process of your models (e.g., learning rate, batch size, number of epochs). Tuning them can significantly impact performance. Here are a couple of effective strategies:\n",
        "\n",
        "Grid Search: Define a range of values for each hyperparameter you want to tune. Grid search systematically tries all possible combinations of these values and evaluates the model's performance for each combination. This is exhaustive but computationally expensive. You can use the GridSearchCV class from sklearn.model_selection for this.\n",
        "Random Search: Randomly sample hyperparameter combinations from a defined search space. This is often more efficient than grid search, especially when dealing with a large number of hyperparameters. You can use the RandomizedSearchCV class from sklearn.model_selection for this.\n",
        "2. Adjusting Training Parameters\n",
        "\n",
        "Besides hyperparameters, there are other training-related aspects you can adjust:\n",
        "\n",
        "Learning Rate: Experiment with different learning rate schedules (e.g., constant, decaying). A learning rate that's too high can lead to instability, while one that's too low can slow down training. You can use learning rate schedulers from the transformers library like get_linear_schedule_with_warmup.\n",
        "Batch Size: Try different batch sizes. Larger batch sizes can speed up training but might require more memory. Smaller batch sizes can help with generalization but might lead to slower training.\n",
        "Number of Epochs: Train for more or fewer epochs. Early stopping can prevent overfitting by monitoring the validation loss and stopping training when it starts to increase. The EarlyStoppingCallback from the transformers library can be used for this.\n",
        "Data Augmentation: If you have a limited dataset, consider data augmentation techniques (e.g., for text data, you could use back-translation or synonym replacement) to increase the diversity of your training data.\n",
        "3. Model-Specific Considerations\n",
        "\n",
        "For the specific models you're using ('distilbert-base-uncased', 'bert-base-uncased', 'roberta-base'), here are some additional considerations:\n",
        "\n",
        "Fine-tuning Layers: You could experiment with fine-tuning only specific layers of the pre-trained models instead of all layers. This can sometimes improve performance and reduce training time.\n",
        "Dropout: Adjust the dropout rate, which is a regularization technique to prevent overfitting. A higher dropout rate might help with generalization.\n",
        "4. Evaluating and Comparing\n",
        "\n",
        "As you make adjustments, carefully evaluate your models using appropriate metrics (accuracy, F1-score, AUC, etc.) on a held-out validation set. Compare the performance of different configurations to select the best one.\n",
        "\n",
        "Applying these techniques to your code:\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Example using RandomizedSearchCV:\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-5, 5e-5, 3e-5],\n",
        "    'per_device_train_batch_size': [8, 16, 32],\n",
        "    'num_train_epochs': [2, 3, 4],\n",
        "}\n",
        "\n",
        "# Wrap the Trainer in a search object\n",
        "search = RandomizedSearchCV(trainer, param_grid, n_iter=5, cv=3)\n",
        "search.fit(tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = search.best_params_\n",
        "\n",
        "# Example using EarlyStoppingCallback:\n",
        "training_args = TrainingArguments(\n",
        "    # ... (your existing training arguments) ...,\n",
        "    load_best_model_at_end=True,  # Load the best model found during training\n",
        "    metric_for_best_model=\"eval_loss\", # Use validation loss for early stopping\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    # ... (your existing trainer setup) ...,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # Stop after 3 epochs without improvement\n",
        ")\n",
        "Körültekintően használja a kódot\n",
        "Remember to experiment and iterate to find the best settings for your specific dataset and task! Let me know if you'd like to explore any of these techniques in more detail or if you have further questions.\n",
        "\n",
        "\n",
        "—------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Let's do the gridsearch\n",
        "\n",
        "Okay, let's implement grid search for hyperparameter tuning in your code. Here's how you can adapt your existing code to incorporate grid search using sklearn.model_selection.GridSearchCV:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# ... (your existing code for loading models, datasets, etc.) ...\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-5, 5e-5, 3e-5],\n",
        "    'per_device_train_batch_size': [8, 16, 32],\n",
        "    'num_train_epochs': [2, 3, 4],\n",
        "}\n",
        "\n",
        "# Create a function to build and train the model\n",
        "def build_and_train_model(learning_rate, per_device_train_batch_size, num_train_epochs):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        report_to=\"none\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, # assuming 'model' is defined earlier\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results['eval_loss']  # Return the evaluation loss for scoring\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=None,  # We're not using a traditional estimator, so set to None\n",
        "    param_grid=param_grid,\n",
        "    scoring=lambda estimator, X, y: build_and_train_model(**estimator.get_params()),  # Custom scoring function\n",
        "    cv=3,  # Number of cross-validation folds (adjust as needed)\n",
        "    n_jobs=-1,  # Use all available cores for parallel processing (optional)\n",
        "    verbose=3, # Increased verbosity to monitor the progress\n",
        ")\n",
        "\n",
        "# Run the grid search\n",
        "grid_search.fit(np.zeros((3, 1)), np.zeros(3))  # Dummy data since we're not using a traditional estimator\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "\n",
        "# Train the final model using the best hyperparameters\n",
        "# ... (Use best_params to create a Trainer and train the final model) ...\n",
        "Körültekintően használja a kódot\n",
        "Explanation of changes\n",
        "\n",
        "Import GridSearchCV: We import the GridSearchCV class from sklearn.model_selection.\n",
        "Define param_grid: This dictionary specifies the hyperparameters and their possible values for the search.\n",
        "Create build_and_train_model: This function encapsulates the process of building and training the model with given hyperparameters. Importantly, it returns the eval_loss, which will be used by GridSearchCV to assess the performance of each hyperparameter combination.\n",
        "Create grid_search: We initialize a GridSearchCV object. Note that estimator is set to None because we're directly using the build_and_train_model function for training. A custom scoring function is defined using a lambda expression, which calls build_and_train_model with the hyperparameters to get the evaluation loss.\n",
        "Run grid_search.fit: We start the grid search process. Since we don't have traditional X and y data for this task, we provide dummy data using np.zeros.\n",
        "Get best_params: After the search is complete, we retrieve the best hyperparameters.\n",
        "Train final model: You would then use these best_params to create a new Trainer instance and train your final model.\n",
        "Remember to adjust the param_grid, cv, and other settings as needed for your specific experiment. Let me know if you have any other questions or need further guidance!\n",
        "\n",
        "\n",
        "—--------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "You said in another conversation that the most reliable metric of evaluation for this project is AUC, f1 and accuracy.\n",
        "\n",
        "Okay, let's refine the grid search to focus on optimizing for AUC, F1-score, and accuracy. Here's how you can modify the code:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "# ... (your existing code for loading models, datasets, etc.) ...\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-5, 5e-5, 3e-5],\n",
        "    'per_device_train_batch_size': [8, 16, 32],\n",
        "    'num_train_epochs': [2, 3, 4],\n",
        "}\n",
        "\n",
        "# Create a function to build, train, and evaluate the model\n",
        "def build_train_and_evaluate_model(learning_rate, per_device_train_batch_size, num_train_epochs):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        report_to=\"none\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    \n",
        "    # Evaluate the model and calculate metrics\n",
        "    predictions = trainer.predict(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)))\n",
        "    predicted_labels = predictions.predictions.argmax(-1)\n",
        "    true_labels = predictions.label_ids\n",
        "    \n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    auc = roc_auc_score(true_labels, predictions.predictions[:, 1])\n",
        "\n",
        "    # Return a dictionary of metrics (you can choose which to optimize for)\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'auc': auc}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=None,\n",
        "    param_grid=param_grid,\n",
        "    scoring=lambda estimator, X, y: build_train_and_evaluate_model(**estimator.get_params())['auc'],  # Optimize for AUC\n",
        "    # or: scoring=lambda estimator, X, y: build_train_and_evaluate_model(**estimator.get_params())['f1'], # Optimize for F1\n",
        "    # or: scoring=lambda estimator, X, y: build_train_and_evaluate_model(**estimator.get_params())['accuracy'], # Optimize for Accuracy\n",
        "    refit=False, # Prevent refitting with the best model\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=3,\n",
        ")\n",
        "\n",
        "# Run the grid search (using dummy data as before)\n",
        "grid_search.fit(np.zeros((3, 1)), np.zeros(3))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "\n",
        "# Train the final model using the best hyperparameters\n",
        "# ... (Use best_params to create a Trainer and train the final model) ...\n",
        "Körültekintően használja a kódot\n",
        "Key Changes\n",
        "\n",
        "build_train_and_evaluate_model function:\n",
        "Now calculates accuracy, F1-score, and AUC after training.\n",
        "Returns a dictionary containing these metrics.\n",
        "scoring in GridSearchCV:\n",
        "Set to use the 'auc' (or 'f1' or 'accuracy') key from the dictionary returned by build_train_and_evaluate_model to optimize for the desired metric.\n",
        "refit=False\n",
        "Prevents GridSearchCV from refitting the model with the best found parameters after the search is complete. We disable it because we only need the hyperparameters and training the best model separately is faster.\n",
        "Now, the grid search will evaluate hyperparameter combinations based on the specified metric (AUC, F1, or accuracy), and you can use the best_params to train your final model with the optimal settings. Remember to select the scoring method you prefer (AUC, F1 or accuracy) and you can change it as you wish in the scoring parameter. I hope this helps you further improve your model's performance!\n",
        "\n",
        "\n",
        "—---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Modify the project so it will use another dataset for the same models. Then compare the models again. Then it will find the differences between the evaluation results of the model on the first and the second datasets. Save the trained models like the current code does. Then load them for the final comparision.\n",
        "\n",
        "Okay, here's how you can modify the project to use another dataset, compare the models, find the differences in evaluation results, and save/load the trained models:\n",
        "\n",
        "1. Choose a New Dataset:\n",
        "\n",
        "First, select a new dataset for text classification. Some popular options available through the datasets library include:\n",
        "ag_news: News categorization dataset.\n",
        "emotion: Emotion classification dataset.\n",
        "rotten_tomatoes: Movie review sentiment classification.\n",
        "2. Update the Code:\n",
        "\n",
        "Replace the load_dataset(\"imdb\") line with your chosen dataset. For example, to use ag_news:\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "Körültekintően használja a kódot\n",
        "Adjust num_labels in AutoModelForSequenceClassification.from_pretrained if the new dataset has a different number of classes.\n",
        "3. Train and Evaluate on the New Dataset:\n",
        "\n",
        "Rerun the training and evaluation code for all three models (DistilBERT, BERT, and RoBERTa) using the new dataset. You'll need to create new Trainer instances and adjust the output directories to avoid overwriting the previous results.\n",
        "4. Compare Models on the New Dataset:\n",
        "\n",
        "Update the comparison code (the part with the pandas DataFrame) to include the evaluation results for the new dataset. You can add new columns to the DataFrame to store these results.\n",
        "5. Find Differences in Evaluation Results:\n",
        "\n",
        "Calculate the differences in evaluation metrics (accuracy, F1-score, etc.) between the two datasets for each model. You can do this by subtracting the metrics obtained on the first dataset from those obtained on the second dataset. You can store these differences in new columns of the DataFrame.\n",
        "6. Save and Load Models:\n",
        "\n",
        "The current code already saves the models. To load them for the final comparison:\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load DistilBERT model\n",
        "model_path_distilbert = \"./my_distilbert_model\"\n",
        "loaded_model_distilbert = AutoModelForSequenceClassification.from_pretrained(model_path_distilbert)\n",
        "loaded_tokenizer_distilbert = AutoTokenizer.from_pretrained(model_path_distilbert)\n",
        "\n",
        "# Load BERT model\n",
        "model_path_bert = \"./my_bert_model\"\n",
        "loaded_model_bert = AutoModelForSequenceClassification.from_pretrained(model_path_bert)\n",
        "loaded_tokenizer_bert = AutoTokenizer.from_pretrained(model_path_bert)\n",
        "\n",
        "# Load RoBERTa model\n",
        "model_path_roberta = \"./my_roberta_model\"\n",
        "loaded_model_roberta = AutoModelForSequenceClassification.from_pretrained(model_path_roberta)\n",
        "loaded_tokenizer_roberta = AutoTokenizer.from_pretrained(model_path_roberta)\n",
        "Körültekintően használja a kódot\n",
        "Example (using ag_news):\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# ... (previous code for model definitions and training) ...\n",
        "\n",
        "# Load new dataset (ag_news)\n",
        "dataset2 = load_dataset(\"ag_news\")\n",
        "\n",
        "# ... (tokenization and training for the new dataset) ...\n",
        "\n",
        "# Compare models on both datasets\n",
        "# ... (create DataFrame with evaluation results for both datasets) ...\n",
        "\n",
        "# Calculate differences in evaluation metrics\n",
        "results_df['accuracy_diff'] = results_df['accuracy_dataset2'] - results_df['accuracy_dataset1']\n",
        "# ... (calculate differences for other metrics) ...\n",
        "\n",
        "# Display the DataFrame with differences\n",
        "display(results_df)\n",
        "\n",
        "# ... (save and load models as described above) ...\n",
        "Körültekintően használja a kódot\n",
        "Remember to adapt the code to your specific needs and the chosen dataset. I hope this helps! Let me know if you have any other questions.\n",
        "\n",
        "\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "Wrap the current code, then write me the same with the rotten_tamotoes\n",
        "\n",
        "Okay, here's the current code wrapped into functions, followed by the modified code to use the rotten_tomatoes dataset:\n",
        "\n",
        "Current Code Wrapped in Functions:\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "def train_and_evaluate_model(model_name, dataset, output_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding=True)\n",
        "\n",
        "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        report_to=\"none\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    predictions = trainer.predict(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)))\n",
        "    predicted_labels = predictions.predictions.argmax(-1)\n",
        "    true_labels = predictions.label_ids\n",
        "    eval_result['eval_accuracy'] = accuracy_score(true_labels, predicted_labels)\n",
        "    eval_result['eval_f1'] = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    eval_result['eval_auc'] = roc_auc_score(true_labels, predictions.predictions[:, 1])\n",
        "\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return eval_result\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset(\"imdb\")\n",
        "dataset2 = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "# Define model names and output directories\n",
        "model_names = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"roberta-base\"]\n",
        "output_dirs = [\"./my_distilbert_model\", \"./my_bert_model\", \"./my_roberta_model\"]\n",
        "\n",
        "# Train and evaluate models on both datasets\n",
        "results = []\n",
        "for model_name, output_dir in zip(model_names, output_dirs):\n",
        "    eval_result1 = train_and_evaluate_model(model_name, dataset1, output_dir + \"_imdb\")\n",
        "    eval_result2 = train_and_evaluate_model(model_name, dataset2, output_dir + \"_rotten_tomatoes\")\n",
        "    results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"eval_result1\": eval_result1,\n",
        "        \"eval_result2\": eval_result2,\n",
        "    })\n",
        "\n",
        "# Create DataFrame for comparison\n",
        "# ...\n",
        "Körültekintően használja a kódot\n",
        "Modified Code for rotten_tomatoes:\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# ... (train_and_evaluate_model function from above) ...\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset(\"imdb\")\n",
        "dataset2 = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "# Define model names and output directories\n",
        "model_names = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"roberta-base\"]\n",
        "output_dirs = [\"./my_distilbert_model\", \"./my_bert_model\", \"./my_roberta_model\"]\n",
        "\n",
        "# Train and evaluate models on both datasets\n",
        "results = []\n",
        "for model_name, output_dir in zip(model_names, output_dirs):\n",
        "    eval_result1 = train_and_evaluate_model(model_name, dataset1, output_dir + \"_imdb\")\n",
        "    eval_result2 = train_and_evaluate_model(model_name, dataset2, output_dir + \"_rotten_tomatoes\")\n",
        "    results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"eval_result1\": eval_result1,\n",
        "        \"eval_result2\": eval_result2,\n",
        "    })\n",
        "\n",
        "# Create DataFrame for comparison\n",
        "metrics = ['eval_loss', 'eval_accuracy', 'eval_f1', 'eval_auc', 'eval_runtime']\n",
        "results_data = []\n",
        "for result in results:\n",
        "    results_data.append({\n",
        "        'Model': result['model_name'],\n",
        "        **{f'{metric}_imdb': result['eval_result1'].get(metric) for metric in metrics},\n",
        "        **{f'{metric}_rotten_tomatoes': result['eval_result2'].get(metric) for metric in metrics},\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Calculate differences in evaluation metrics\n",
        "for metric in metrics:\n",
        "    results_df[f'{metric}_diff'] = results_df[f'{metric}_rotten_tomatoes'] - results_df[f'{metric}_imdb']\n",
        "\n",
        "# Display the DataFrame with differences\n",
        "display(results_df)\n",
        "Körültekintően használja a kódot\n",
        "Changes:\n",
        "\n",
        "dataset2: Now loads rotten_tomatoes.\n",
        "Output Directories: Modified to include dataset names (_imdb, _rotten_tomatoes) to avoid overwriting.\n",
        "DataFrame: Updated to include results for both datasets and calculate differences.\n",
        "I hope this is helpful! Feel free to ask if you have any other questions.\n",
        "\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "integrate a saving method to the code so next time it will load the trained model from saved files.\n",
        "\n",
        "You can modify your train_and_evaluate_model function to check if a model already exists in the output_dir. If it does, load it instead of training a new one.\n",
        "Here's an updated version of your function:\n",
        "library_add\n",
        "content_copy\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def train_and_evaluate_model(model_name, dataset, output_dir):\n",
        "    # Check if model and tokenizer already exist\n",
        "    if os.path.exists(output_dir) and os.path.exists(os.path.join(output_dir, 'pytorch_model.bin')) and os.path.exists(os.path.join(output_dir, 'tokenizer_config.json')):\n",
        "        print(f\"Loading existing model from {output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "    else:\n",
        "        print(f\"Training new model and saving to {output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "        tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            report_to=\"none\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=2,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "            eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        trainer.save_model(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Evaluation part (same as before)\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=output_dir, report_to=\"none\"), # Use minimal args for evaluation\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "\n",
        "    eval_result = trainer.evaluate()\n",
        "    predictions = trainer.predict(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)))\n",
        "    predicted_labels = predictions.predictions.argmax(-1)\n",
        "    true_labels = predictions.label_ids\n",
        "    eval_result['eval_accuracy'] = accuracy_score(true_labels, predicted_labels)\n",
        "    eval_result['eval_f1'] = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    eval_result['eval_auc'] = roc_auc_score(true_labels, predictions.predictions[:, 1])\n",
        "\n",
        "    return eval_result\n",
        "Körültekintően használja a kódot\n",
        "This modification checks for the presence of the pytorch_model.bin and tokenizer_config.json files within the specified output_dir. If these files exist, it assumes a trained model is present and loads it directly using from_pretrained(output_dir). Otherwise, it proceeds with training the model as before and saves it.\n",
        "Note that the evaluation part is now done separately after either training or loading the model. This is because the Trainer object used for training might not be needed for just evaluation, and we can create a new one with minimal arguments specifically for evaluation.\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Add a chat agent to code so the user writes a review and the agent returns the classification wether the review is positive or negative.\n",
        "\n",
        "To add a chat agent that classifies user reviews, you can integrate a simple input loop and use one of your trained models to make predictions. Here's how you can do it using the trained DistilBERT model as an example:\n",
        "First, ensure you have trained and saved at least one model using the train_and_evaluate_model function. For this example, we'll assume the DistilBERT model trained on the IMDB dataset is saved in ./my_distilbert_model_imdb.\n",
        "library_add\n",
        "content_copy\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset\n",
        "model_path = \"./my_distilbert_model_imdb\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Define a function to classify a single review\n",
        "def classify_review(review_text, tokenizer, model):\n",
        "    # Tokenize the input review\n",
        "    inputs = tokenizer(review_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Get model predictions\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "    # Map prediction to sentiment\n",
        "    sentiment = \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "    return sentiment\n",
        "\n",
        "# Implement the chat agent loop\n",
        "print(\"Welcome to the Review Sentiment Classifier!\")\n",
        "print(\"Type your review or 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_review = input(\"Enter your review: \")\n",
        "    if user_review.lower() == 'quit':\n",
        "        break\n",
        "    sentiment = classify_review(user_review, tokenizer, model)\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "\n",
        "print(\"Chat agent closed.\")\n",
        "Körültekintően használja a kódot\n",
        "Explanation:\n",
        "Load Model and Tokenizer: We load the trained distilbert-base-uncased model and its tokenizer from the specified directory (./my_distilbert_model_imdb). You can change this path to load any of the other trained models.\n",
        "classify_review Function: This function takes a review text, the tokenizer, and the model as input.\n",
        "It tokenizes the review text using the loaded tokenizer, ensuring it's in the format expected by the model. return_tensors=\"pt\" returns PyTorch tensors.\n",
        "It passes the tokenized inputs to the model to get predictions.\n",
        "outputs.logits contains the raw prediction scores. We use argmax(-1) to get the index of the highest score, which corresponds to the predicted class (0 for negative, 1 for positive in this case, assuming the labels are mapped this way during training).\n",
        "Finally, it maps the predicted class index to the sentiment string (\"Positive\" or \"Negative\").\n",
        "Chat Agent Loop:\n",
        "It enters an infinite loop that continues until the user types 'quit'.\n",
        "Inside the loop, it prompts the user to enter a review.\n",
        "If the input is not 'quit', it calls the classify_review function to get the sentiment.\n",
        "It prints the predicted sentiment.\n",
        "To use this, you would run this code cell after the cell that trains and saves your models. The chat agent will then start, allowing you to enter reviews and get sentiment classifications.\n",
        "\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Can this code be used for multi-class classification if I change the datasets and models?\n",
        "\n",
        "Yes, this code can be adapted for multi-class classification by changing the datasets and adjusting the number of labels in the model.\n",
        "Here's how you would need to modify the code:\n",
        "Dataset Selection: You would need to load a dataset that is suitable for multi-class classification. This dataset should contain text data (like movie descriptions) and corresponding labels for different classes (like genres).\n",
        "Number of Labels: In the train_and_evaluate_model function, you would need to change the num_labels argument when loading the model to the number of distinct classes in your multi-class dataset.\n",
        "Evaluation Metrics: You might need to adjust the evaluation metrics. While accuracy is still relevant, you might consider using metrics like precision, recall, and a weighted or macro average for f1_score to account for potential class imbalance.\n",
        "Classification Logic: In the classify_review function (or a similar function for inference), you would need to map the model's output (which will be a probability distribution over the classes) to the predicted class label.\n",
        "By making these changes, you can repurpose this code for various multi-class text classification tasks.\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "How to make a good visualization of this project?\n",
        "\n",
        "You can visualize the evaluation results from the results_df DataFrame. Here's how you can do it:\n",
        "Import Visualization Libraries: Import libraries like matplotlib.pyplot or seaborn for creating plots.\n",
        "Prepare Data for Visualization: You'll need to structure your data to make plotting easier. You can melt the DataFrame or select specific columns.\n",
        "Create Plots: Use bar plots, line plots, or other suitable plot types to compare the evaluation metrics (accuracy, F1-score, AUC, etc.) for each model on both datasets.\n",
        "Here's an example using matplotlib.pyplot to create bar plots for comparing accuracy:\n",
        "library_add\n",
        "content_copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming results_df is already created as in your notebook\n",
        "\n",
        "metrics = ['eval_accuracy', 'eval_f1', 'eval_auc']\n",
        "datasets = ['imdb', 'rotten_tomatoes']\n",
        "\n",
        "for metric in metrics:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bar_width = 0.35\n",
        "    index = range(len(results_df['Model']))\n",
        "\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        ax.bar([p + bar_width * i for p in index], results_df[f'{metric}_{dataset}'], bar_width, label=dataset)\n",
        "\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison by Dataset')\n",
        "    ax.set_xticks([p + bar_width / 2 for p in index])\n",
        "    ax.set_xticklabels(results_df['Model'])\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "Körültekintően használja a kódot\n",
        "This code will generate separate bar plots for each metric, comparing the performance of each model on both datasets. You can adapt this to visualize other metrics as well.\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "start y axis value from 0.6\n",
        "\n",
        "To set the starting value of the y-axis in your matplotlib plots, you can use the ax.set_ylim() method. This allows you to specify the minimum and maximum values for the y-axis.\n",
        "Here's how you can modify the visualization code to set the y-axis starting from 0.6 for metrics like accuracy, F1-score, and AUC, where this range is relevant:\n",
        "library_add\n",
        "content_copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming results_df is already created as in your notebook\n",
        "\n",
        "metrics = ['eval_accuracy', 'eval_f1', 'eval_auc'] # Metrics where 0.6 might be a relevant lower bound\n",
        "datasets = ['imdb', 'rotten_tomatoes']\n",
        "\n",
        "for metric in metrics:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bar_width = 0.35\n",
        "    index = range(len(results_df['Model']))\n",
        "\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        ax.bar([p + bar_width * i for p in index], results_df[f'{metric}_{dataset}'], bar_width, label=dataset)\n",
        "\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison by Dataset')\n",
        "    ax.set_xticks([p + bar_width / 2 for p in index])\n",
        "    ax.set_xticklabels(results_df['Model'])\n",
        "    ax.legend()\n",
        "\n",
        "    # Set the y-axis limits\n",
        "    ax.set_ylim(0.6, 1.0) # Set lower limit to 0.6 and upper limit to 1.0 (for metrics between 0 and 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# For metrics like eval_loss or eval_runtime, setting a lower limit of 0.6 might not be appropriate.\n",
        "# You would handle those visualizations separately or adjust the ylim accordingly.\n",
        "# Example for loss (adjust limits as needed):\n",
        "# fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# # ... plotting loss ...\n",
        "# ax.set_ylim(min_loss_value, max_loss_value)\n",
        "# plt.show()\n",
        "Körültekintően használja a kódot\n",
        "In this code:\n",
        "We focus on metrics where a lower bound of 0.6 makes sense (accuracy, F1, AUC).\n",
        "ax.set_ylim(0.6, 1.0) is used to set the lower limit of the y-axis to 0.6 and the upper limit to 1.0, which is a typical upper bound for these metrics.\n",
        "You would need to handle metrics like 'eval_loss' or 'eval_runtime' separately, as their value ranges are different and a lower limit of 0.6 would likely not be appropriate. You would set the ylim for those plots based on the actual range of your loss or runtime values.\n",
        "By setting a custom y-axis limit, you can focus the visualization on the range of values where the interesting differences occur, making it easier to compare models within that range. However, be mindful that this can sometimes make small differences appear larger than they are.\n",
        "\n",
        "—-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Write me a code that input the test texts above to chat when the chat starts.\n",
        "\n",
        "Testing the Chat Agent with Predefined Reviews\n",
        "This code block modifies the previous chat agent to automatically input the provided example reviews when it starts. It stores the reviews in a list and iterates through them, feeding each one to the classify_review function and printing the result, simulating a user entering these reviews.\n",
        "library_add\n",
        "content_copy\n",
        "# Define the list of test reviews\n",
        "test_reviews = [\n",
        "    \"I absolutely loved this movie! The story was captivating and the performances were outstanding.\",\n",
        "    \"This was one of the worst films I've seen. The plot made no sense and the acting was terrible.\",\n",
        "    \"It wasn’t perfect, but I found it enjoyable overall and would probably watch it again.\",\n",
        "    \"The movie had a few interesting moments, but overall it felt too slow and uninspired.\",\n",
        "    \"The actors did their best with a terrible script.\",\n",
        "    \"I expected more, but it wasn’t entirely bad.\",\n",
        "    \"Oh great, another movie about saving the world. Just what we needed.\",\n",
        "    \"Fantastic... if you enjoy falling asleep halfway through.\",\n",
        "    \"The visuals were amazing, but the storyline was a disaster.\",\n",
        "    \"Horrible pacing, yet I couldn’t stop watching because of the lead actor’s performance.\",\n",
        "    \"Not bad.\",\n",
        "    \"It was fine.\",\n",
        "    \"This movie completely changed my life!\",\n",
        "    \"This film runs for 120 minutes and contains scenes of dialogue.\",\n",
        "    \"As a cinematographer, I appreciated the dynamic lighting choices.\",\n",
        "    \"This reminded me of 90s European art films.\",\n",
        "    \"I rly enjoyed this movy, gr8 job!!\",\n",
        "    \"It was meh. Not my typa thing.\",\n",
        "    \"This movie was so chill, nagyon tetszett!\",\n",
        "    \"Not bad, but la musique was distracting.\"\n",
        "]\n",
        "\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset\n",
        "model_path = \"./my_bert_model_imdb\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Define the function to classify a single review (copied from previous section)\n",
        "def classify_review(review_text, tokenizer, model):\n",
        "    inputs = tokenizer(review_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "    sentiment = \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "    return sentiment\n",
        "\n",
        "# Implement the automated review processing\n",
        "print(\"Welcome to the Review Sentiment Classifier (Automated Test)! \\n\")\n",
        "\n",
        "for review in test_reviews:\n",
        "    print(f\"Processing review: \\\"{review}\\\"\")\n",
        "    sentiment = classify_review(review, tokenizer, model)\n",
        "    print(f\"Sentiment: {sentiment}\\n\")\n",
        "\n",
        "print(\"Automated testing complete.\")\n",
        "Körültekintően használja a kódot\n",
        "First, the provided test reviews are stored in a Python list called test_reviews.\n",
        "The trained model and tokenizer are loaded the same way as in the previous section.\n",
        "The classify_review function is also included again, as it's needed to process each review.\n",
        "Instead of a while True loop waiting for user input, a for loop iterates through each review in the test_reviews list.\n",
        "For each review, it first prints the review text to show which one is being processed.\n",
        "Then, it calls the classify_review function to get the sentiment prediction for that review.\n",
        "Finally, it prints the predicted sentiment.\n",
        "A blank line (\\n) is added after each result to make the output clearer.\n",
        "After the loop finishes processing all the reviews, a message indicates that the automated testing is complete.\n",
        "\n",
        "\n",
        "—-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Modify the code so it will save and load model files to my google drive so they won't be deleted at the end of the session.\n",
        "\n",
        "Okay, I can help you modify your code to save and load your trained models from Google Drive.\n",
        "Here's how you can adjust your code:\n",
        "Mount Google Drive: You've already included the code to mount Google Drive, which is the first step.\n",
        "Update Output Directories: Change the output_dirs variable to point to locations within your mounted Google Drive. Make sure these directories exist in your Drive.\n",
        "Modify train_and_evaluate_model: Update the function to use the new Google Drive paths for saving and loading.\n",
        "Here's the modified code:\n",
        "library_add\n",
        "content_copy\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!pip install transformers datasets\n",
        "!pip install --upgrade datasets transformers fsspec\n",
        "# %%\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive (ensure this is run first)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def train_and_evaluate_model(model_name, dataset, output_dir):\n",
        "    # Construct the full path within Google Drive\n",
        "    drive_output_dir = os.path.join('/content/drive/MyDrive/', output_dir)\n",
        "\n",
        "    # Check if model and tokenizer already exist in Google Drive\n",
        "    if os.path.exists(drive_output_dir) and os.path.exists(os.path.join(drive_output_dir, 'config.json')) \\\n",
        "    and os.path.exists(os.path.join(drive_output_dir, 'tokenizer_config.json')):\n",
        "        print(f\"Loading existing model from {drive_output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(drive_output_dir)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(drive_output_dir)\n",
        "    else:\n",
        "        print(f\"Training new model and saving to {drive_output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "        tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=drive_output_dir, # Save to Google Drive\n",
        "            report_to=\"none\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=2,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "            eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # Create the output directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(drive_output_dir, exist_ok=True)\n",
        "\n",
        "        trainer.save_model(drive_output_dir) # Save to Google Drive\n",
        "        tokenizer.save_pretrained(drive_output_dir) # Save to Google Drive\n",
        "\n",
        "    # Evaluation part (same as before)\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=drive_output_dir, report_to=\"none\"), # Use the drive path for evaluation output as well\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    eval_result = trainer.evaluate()\n",
        "    predictions = trainer.predict(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)))\n",
        "    predicted_labels = predictions.predictions.argmax(-1)\n",
        "    true_labels = predictions.label_ids\n",
        "    eval_result['eval_accuracy'] = accuracy_score(true_labels, predicted_labels)\n",
        "    eval_result['eval_f1'] = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    eval_result['eval_auc'] = roc_auc_score(true_labels, predictions.predictions[:, 1])\n",
        "\n",
        "\n",
        "    return eval_result\n",
        "# %%\n",
        "# Load datasets\n",
        "dataset1 = load_dataset(\"imdb\")\n",
        "dataset2 = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "# Define model names and output directories (relative to MyDrive/)\n",
        "model_names = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"roberta-base\"]\n",
        "output_dirs = [\"my_distilbert_model\", \"my_bert_model\", \"my_roberta_model\"] # These will be within /content/drive/MyDrive/\n",
        "\n",
        "# Train and evaluate models on both datasets, saving the models to Google Drive\n",
        "results = []\n",
        "for model_name, output_dir in zip(model_names, output_dirs):\n",
        "    eval_result1 = train_and_evaluate_model(model_name, dataset1, output_dir + \"_imdb\")\n",
        "    eval_result2 = train_and_evaluate_model(model_name, dataset2, output_dir + \"_rotten_tomatoes\")\n",
        "    results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"eval_result1\": eval_result1,\n",
        "        \"eval_result2\": eval_result2,\n",
        "    })\n",
        "# %%\n",
        "# Create DataFrame for comparison\n",
        "metrics = ['eval_loss', 'eval_accuracy', 'eval_f1', 'eval_auc', 'eval_runtime']\n",
        "results_data = []\n",
        "for result in results:\n",
        "    results_data.append({\n",
        "        'Model': result['model_name'],\n",
        "        **{f'{metric}_imdb': result['eval_result1'].get(metric) for metric in metrics},\n",
        "        **{f'{metric}_rotten_tomatoes': result['eval_result2'].get(metric) for metric in metrics},\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Display the DataFrame with differences\n",
        "display(results_df)\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming results_df is already created as in your notebook\n",
        "\n",
        "metrics = ['eval_accuracy', 'eval_f1', 'eval_auc'] # Metrics where 0.6 might be a relevant lower bound\n",
        "datasets = ['imdb', 'rotten_tomatoes']\n",
        "\n",
        "for metric in metrics:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bar_width = 0.35\n",
        "    index = range(len(results_df['Model']))\n",
        "\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        ax.bar([p + bar_width * i for p in index], results_df[f'{metric}_{dataset}'], bar_width, label=dataset)\n",
        "\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison by Dataset')\n",
        "    ax.set_xticks([p + bar_width / 2 for p in index])\n",
        "    ax.set_xticklabels(results_df['Model'])\n",
        "    ax.legend()\n",
        "\n",
        "    # Set the y-axis limits\n",
        "    ax.set_ylim(0.7, 0.95) # Set lower limit to 0.6 and upper limit to 1.0 (for metrics between 0 and 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# %% [markdown]\n",
        "# ## Text for the chat to test the model:\n",
        "#\n",
        "# **✅ 1. Positive**\n",
        "#\n",
        "# \"I absolutely loved this movie! The story was captivating and the performances were outstanding.\"\n",
        "#\n",
        "#\n",
        "# **❌ 2. Negative**\n",
        "#\n",
        "# \"This was one of the worst films I've seen. The plot made no sense and the acting was terrible.\"\n",
        "#\n",
        "#\n",
        "# **⚖️ 3. Neutral, but slightly positive (challenging)**\n",
        "#\n",
        "# \"It wasn’t perfect, but I found it enjoyable overall and would probably watch it again.\"\n",
        "#\n",
        "# **⚖️ 4. Neutral, but slightly negative (challenging)**\n",
        "#\n",
        "# \"The movie had a few interesting moments, but overall it felt too slow and uninspired.\"\n",
        "#\n",
        "#\n",
        "# **✅ 5. Ambiguous Sentiment**\n",
        "# Test borderline or contradictory opinions that may confuse the model:\n",
        "#\n",
        "# \"The actors did their best with a terrible script.\"\n",
        "#\n",
        "# \"I expected more, but it wasn’t entirely bad.\"\n",
        "#\n",
        "# **✅ 6. Sarcasm and Irony**\n",
        "#\n",
        "# \"Oh great, another movie about saving the world. Just what we needed.\"\n",
        "#\n",
        "# \"Fantastic... if you enjoy falling asleep halfway through.\"\n",
        "#\n",
        "# **✅ 7. Mixed Sentiment**\n",
        "# Include both strong positive and negative signals:\n",
        "#\n",
        "# \"The visuals were amazing, but the storyline was a disaster.\"\n",
        "#\n",
        "# \"Horrible pacing, yet I couldn’t stop watching because of the lead actor’s performance.\"\n",
        "#\n",
        "# **✅ 8. Short and Vague Reviews**\n",
        "#\n",
        "# \"Not bad.\" (Positive or neutral?)\n",
        "#\n",
        "# \"It was fine.\" (Very ambiguous.)\n",
        "#\n",
        "# **✅ 9. Highly Emotional vs. Flat Tone**\n",
        "#\n",
        "# \"This movie completely changed my life!\" (Clearly positive and emotional)\n",
        "#\n",
        "# \"This film runs for 120 minutes and contains scenes of dialogue.\" (Neutral)\n",
        "#\n",
        "# **✅ 10. Domain-Specific or Off-topic Language**\n",
        "# Test for generalization beyond typical review words:\n",
        "#\n",
        "# \"As a cinematographer, I appreciated the dynamic lighting choices.\"\n",
        "#\n",
        "# \"This reminded me of 90s European art films.\"\n",
        "#\n",
        "# **✅ 11. Typos and Informal Language**\n",
        "#\n",
        "# \"I rly enjoyed this movy, gr8 job!!\"\n",
        "#\n",
        "# \"It was meh. Not my typa thing.\"\n",
        "#\n",
        "# **✅ 12. Foreign Words / Code-Switching**\n",
        "#\n",
        "# \"This movie was so chill, nagyon tetszett!\"\n",
        "#\n",
        "# \"Not bad, but la musique was distracting.\"\n",
        "#\n",
        "#\n",
        "#\n",
        "# %%\n",
        "# Define the list of test reviews\n",
        "test_reviews = [\n",
        "    \"I absolutely loved this movie! The story was captivating and the performances were outstanding.\",\n",
        "    \"This was one of the worst films I've seen. The plot made no sense and the acting was terrible.\",\n",
        "    \"It wasn’t perfect, but I found it enjoyable overall and would probably watch it again.\",\n",
        "    \"The movie had a few interesting moments, but overall it felt too slow and uninspired.\",\n",
        "    \"The actors did their best with a terrible script.\",\n",
        "    \"I expected more, but it wasn’t entirely bad.\",\n",
        "    \"Oh great, another movie about saving the world. Just what we needed.\",\n",
        "    \"Fantastic... if you enjoy falling asleep halfway through.\",\n",
        "    \"The visuals were amazing, but the storyline was a disaster.\",\n",
        "    \"Horrible pacing, yet I couldn’t stop watching because of the lead actor’s performance.\",\n",
        "    \"Not bad.\",\n",
        "    \"It was fine.\",\n",
        "    \"This movie completely changed my life!\",\n",
        "    \"This film runs for 120 minutes and contains scenes of dialogue.\",\n",
        "    \"As a cinematographer, I appreciated the dynamic lighting choices.\",\n",
        "    \"This reminded me of 90s European art films.\",\n",
        "    \"I rly enjoyed this movy, gr8 job!!\",\n",
        "    \"It was meh. Not my typa thing.\",\n",
        "    \"This movie was so chill, nagyon tetszett!\",\n",
        "    \"Not bad, but la musique was distracting.\"\n",
        "]\n",
        "\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset from Google Drive\n",
        "model_path = \"/content/drive/MyDrive/my_bert_model_imdb\" # Updated path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Define the function to classify a single review (copied from previous section)\n",
        "def classify_review(review_text, tokenizer, model):\n",
        "    inputs = tokenizer(review_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "    sentiment = \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "    return sentiment\n",
        "\n",
        "# Implement the automated review processing\n",
        "print(\"Welcome to the Review Sentiment Classifier (Automated Test)! \\n\")\n",
        "\n",
        "for review in test_reviews:\n",
        "    print(f\"Processing review: \\\"{review}\\\"\")\n",
        "    sentiment = classify_review(review, tokenizer, model)\n",
        "    print(f\"Sentiment: {sentiment}\\n\")\n",
        "\n",
        "print(\"Automated testing complete.\")\n",
        "# %%\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset from Google Drive\n",
        "model_path = \"/content/drive/MyDrive/my_bert_model_imdb\" # Updated path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Define a function to classify a single review\n",
        "def classify_review(review_text, tokenizer, model):\n",
        "    # Tokenize the input review\n",
        "    inputs = tokenizer(review_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Get model predictions\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "    # Map prediction to sentiment\n",
        "    sentiment = \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "    return sentiment\n",
        "\n",
        "# Implement the chat agent loop\n",
        "print(\"Welcome to the Review Sentiment Classifier!\")\n",
        "print(\"Type your review or 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_review = input(\"Enter your review: \")\n",
        "    if user_review.lower() == 'quit':\n",
        "        break\n",
        "    sentiment = classify_review(user_review, tokenizer, model)\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "\n",
        "print(\"Chat agent closed.\")\n",
        "Körültekintően használja a kódot\n",
        "Explanation of Changes:\n",
        "drive.mount('/content/drive'): This line is already in your notebook and is necessary to access your Google Drive. Make sure this cell is executed before you try to save or load models from Drive.\n",
        "drive_output_dir = os.path.join('/content/drive/MyDrive/', output_dir): Inside the train_and_evaluate_model function, we create a full path to the desired location within your Google Drive. /content/drive/MyDrive/ is the standard path to your main Google Drive folder after mounting.\n",
        "Using drive_output_dir: We use drive_output_dir consistently for checking if the model exists, for the output_dir in TrainingArguments, and for trainer.save_model and tokenizer.save_pretrained.\n",
        "Updated model_path in later cells: In the cells where you load the trained model for testing and the chat agent, the model_path is updated to point to the location in Google Drive (e.g., /content/drive/MyDrive/my_bert_model_imdb).\n",
        "Now, when you run the training code, the models will be saved to the specified directories within your Google Drive. In subsequent sessions, as long as you mount your Drive, the code will load the existing models from those locations, preventing retraining.\n",
        "\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "Provide me the differences between the original and the modified code.\n",
        "\n",
        "Okay, here's a breakdown of the differences between your original code and the modified version for saving and loading models from Google Drive:\n",
        "Original Code:\n",
        "Saving and Loading Location: Models were saved and loaded to directories within the Colab environment's temporary storage (e.g., ./my_distilbert_model). This storage is deleted when the Colab session ends.\n",
        "Modified Code:\n",
        "Saving and Loading Location: Models are saved and loaded to directories within your mounted Google Drive (e.g., /content/drive/MyDrive/my_distilbert_model). This storage is persistent across Colab sessions.\n",
        "Mounting Google Drive: The code explicitly includes from google.colab import drive and drive.mount('/content/drive') to enable access to Google Drive.\n",
        "Path Construction: Within the train_and_evaluate_model function, a new variable drive_output_dir is created using os.path.join('/content/drive/MyDrive/', output_dir). This variable holds the full path to the directory within Google Drive where the model will be stored.\n",
        "Using Drive Paths:\n",
        "The os.path.exists() checks for existing models now point to drive_output_dir.\n",
        "The output_dir parameter in TrainingArguments is set to drive_output_dir.\n",
        "trainer.save_model() and tokenizer.save_pretrained() use drive_output_dir to save the model and tokenizer to Google Drive.\n",
        "The model_path variable in the later cells (for automated testing and the chat agent) is updated to reflect the Google Drive location (e.g., /content/drive/MyDrive/my_bert_model_imdb).\n",
        "In essence, the core logic of your training and evaluation remains the same. The primary difference is the redirection of file operations (saving and loading models) from the temporary Colab environment to your persistent Google Drive storage.\n",
        "\n",
        "—-----------------------------------------------------------------------------------\n",
        "the exact code snippets\n",
        "\n",
        "Okay, here are the exact code snippets that show the differences between the original and modified code, focusing on the changes made to save and load models to Google Drive.\n",
        "1. Mounting Google Drive (Added to the top section, after imports):\n",
        "Original (Not present at this location):\n",
        "library_add\n",
        "content_copy\n",
        "# No code for mounting drive here\n",
        "Körültekintően használja a kódot\n",
        "Modified:\n",
        "library_add\n",
        "content_copy\n",
        "# %%\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive # Added import\n",
        "\n",
        "# Mount Google Drive (ensure this is run first)\n",
        "drive.mount('/content/drive') # Added mounting code\n",
        "Körültekintően használja a kódot\n",
        "2. Modification within the train_and_evaluate_model function:\n",
        "Original:\n",
        "library_add\n",
        "content_copy\n",
        "def train_and_evaluate_model(model_name, dataset, output_dir):\n",
        "    # Check if model and tokenizer already exist\n",
        "    if os.path.exists(output_dir) and os.path.exists(os.path.join(output_dir, 'config.json')) \\\n",
        "    and os.path.exists(os.path.join(output_dir, 'tokenizer_config.json')):\n",
        "        print(f\"Loading existing model from {output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "    else:\n",
        "        print(f\"Training new model and saving to {output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "        # ... (rest of the training code) ...\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        trainer.save_model(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Evaluation part (same as before)\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=output_dir, report_to=\"none\"), # Use minimal args for evaluation\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    # ... (rest of evaluation code) ...\n",
        "Körültekintően használja a kódot\n",
        "Modified:\n",
        "library_add\n",
        "content_copy\n",
        "def train_and_evaluate_model(model_name, dataset, output_dir):\n",
        "    # Construct the full path within Google Drive\n",
        "    drive_output_dir = os.path.join('/content/drive/MyDrive/', output_dir) # Added\n",
        "\n",
        "    # Check if model and tokenizer already exist in Google Drive\n",
        "    if os.path.exists(drive_output_dir) and os.path.exists(os.path.join(drive_output_dir, 'config.json')) \\\n",
        "    and os.path.exists(os.path.join(drive_output_dir, 'tokenizer_config.json')):\n",
        "        print(f\"Loading existing model from {drive_output_dir}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(drive_output_dir) # Changed\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(drive_output_dir) # Changed\n",
        "    else:\n",
        "        print(f\"Training new model and saving to {drive_output_dir}\") # Changed\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "        tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=drive_output_dir, # Save to Google Drive # Changed\n",
        "            report_to=\"none\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=2,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "            eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # Create the output directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(drive_output_dir, exist_ok=True) # Changed\n",
        "\n",
        "        trainer.save_model(drive_output_dir) # Save to Google Drive # Changed\n",
        "        tokenizer.save_pretrained(drive_output_dir) # Save to Google Drive # Changed\n",
        "\n",
        "    # Evaluation part (same as before)\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=drive_output_dir, report_to=\"none\"), # Use the drive path for evaluation output as well # Changed\n",
        "        eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
        "    )\n",
        "\n",
        "    # ... (rest of evaluation code) ...\n",
        "Körültekintően használja a kódot\n",
        "3. Modification of output_dirs:\n",
        "Original:\n",
        "library_add\n",
        "content_copy\n",
        "# Define model names and output directories\n",
        "model_names = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"roberta-base\"]\n",
        "output_dirs = [\"./my_distilbert_model\", \"./my_bert_model\", \"./my_roberta_model\"] # Relative paths\n",
        "Körültekintően használja a kódot\n",
        "Modified:\n",
        "library_add\n",
        "content_copy\n",
        "# Define model names and output directories (relative to MyDrive/)\n",
        "model_names = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"roberta-base\"]\n",
        "output_dirs = [\"my_distilbert_model\", \"my_bert_model\", \"my_roberta_model\"] # These will be within /content/drive/MyDrive/ # Changed - removed leading './'\n",
        "Körültekintően használja a kódot\n",
        "4. Modification of model_path in the automated test and chat agent sections:\n",
        "Original:\n",
        "library_add\n",
        "content_copy\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset\n",
        "model_path = \"./my_bert_model_imdb\" # Relative path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "Körültekintően használja a kódot\n",
        "Modified:\n",
        "library_add\n",
        "content_copy\n",
        "# Load the trained DistilBERT model and tokenizer for IMDB dataset from Google Drive\n",
        "model_path = \"/content/drive/MyDrive/my_bert_model_imdb\" # Updated path # Changed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "Körültekintően használja a kódot\n",
        "These snippets highlight the key lines that were added or modified to implement saving and loading from Google Drive.\n",
        "  —------------------------------------------------------\n",
        "\n",
        "Rewirte it so the user can choose wether starting automated testing or the user chat\n",
        "\n",
        "Okay, let's modify the code to allow the user to choose between running the automated tests or starting the interactive chat agent.\n",
        "Here's the rewritten code with the user choice functionality:\n",
        "library_add\n",
        "content_copy\n",
        "# ... (previous imports and train_and_evaluate_model function) ...\n",
        "\n",
        "# Define the list of test reviews\n",
        "test_reviews = [\n",
        "    \"I absolutely loved this movie! The story was captivating and the performances were outstanding.\",\n",
        "    \"This was one of the worst films I've seen. The plot made no sense and the acting was terrible.\",\n",
        "    \"It wasn’t perfect, but I found it enjoyable overall and would probably watch it again.\",\n",
        "    \"The movie had a few interesting moments, but overall it felt too slow and uninspired.\",\n",
        "    \"The actors did their best with a terrible script.\",\n",
        "    \"I expected more, but it wasn’t entirely bad.\",\n",
        "    \"Oh great, another movie about saving the world. Just what we needed.\",\n",
        "    \"Fantastic... if you enjoy falling asleep halfway through.\",\n",
        "    \"The visuals were amazing, but the storyline was a disaster.\",\n",
        "    \"Horrible pacing, yet I couldn’t stop watching because of the lead actor’s performance.\",\n",
        "    \"Not bad.\",\n",
        "    \"It was fine.\",\n",
        "    \"This movie completely changed my life!\",\n",
        "    \"This film runs for 120 minutes and contains scenes of dialogue.\",\n",
        "    \"As a cinematographer, I appreciated the dynamic lighting choices.\",\n",
        "    \"This reminded me of 90s European art films.\",\n",
        "    \"I rly enjoyed this movy, gr8 job!!\",\n",
        "    \"It was meh. Not my typa thing.\",\n",
        "    \"This movie was so chill, nagyon tetszett!\",\n",
        "    \"Not bad, but la musique was distracting.\"\n",
        "]\n",
        "\n",
        "# Function to load the trained model and tokenizer\n",
        "def load_sentiment_model(model_path):\n",
        "    \"\"\"Loads a trained sentiment analysis model and tokenizer.\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {model_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to classify a list of reviews\n",
        "def classify_reviews(reviews, tokenizer, model):\n",
        "    \"\"\"Classifies a list of reviews using the provided model and tokenizer.\"\"\"\n",
        "    results = []\n",
        "    for review in reviews:\n",
        "        inputs = tokenizer(review, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)\n",
        "        sentiment = \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "        results.append({\"review\": review, \"sentiment\": sentiment})\n",
        "    return results\n",
        "\n",
        "# --- Main Program Flow ---\n",
        "\n",
        "print(\"Welcome to the Review Sentiment Classifier!\")\n",
        "print(\"Choose an option:\")\n",
        "print(\"1. Run Automated Tests\")\n",
        "print(\"2. Start User Chat Agent\")\n",
        "print(\"3. Exit\")\n",
        "\n",
        "while True:\n",
        "    choice = input(\"Enter your choice (1, 2, or 3): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        # --- Automated Testing ---\n",
        "        print(\"\\nRunning Automated Tests...\\n\")\n",
        "\n",
        "        # Load the trained BERT model for IMDB dataset\n",
        "        model_path = \"/content/drive/MyDrive/my_bert_model_imdb\"\n",
        "        tokenizer, model = load_sentiment_model(model_path)\n",
        "\n",
        "        if tokenizer and model:\n",
        "            automated_test_results = classify_reviews(test_reviews, tokenizer, model)\n",
        "            for result in automated_test_results:\n",
        "                print(f\"Processing review: \\\"{result['review']}\\\"\")\n",
        "                print(f\"Sentiment: {result['sentiment']}\\n\")\n",
        "        else:\n",
        "            print(\"Skipping automated testing due to model loading error.\")\n",
        "\n",
        "    elif choice == '2':\n",
        "        # --- User Chat Agent ---\n",
        "        print(\"\\nStarting User Chat Agent...\")\n",
        "        print(\"Type your review or 'quit' to exit.\")\n",
        "\n",
        "        # Load the trained BERT model for IMDB dataset\n",
        "        model_path = \"/content/drive/MyDrive/my_bert_model_imdb\"\n",
        "        tokenizer, model = load_sentiment_model(model_path)\n",
        "\n",
        "        if tokenizer and model:\n",
        "            while True:\n",
        "                user_review = input(\"Enter your review: \")\n",
        "                if user_review.lower() == 'quit':\n",
        "                    break\n",
        "                # Classify a single review by putting it in a list\n",
        "                chat_result = classify_reviews([user_review], tokenizer, model)\n",
        "                print(f\"Sentiment: {chat_result[0]['sentiment']}\")\n",
        "        else:\n",
        "            print(\"Skipping chat agent due to model loading error.\")\n",
        "\n",
        "    elif choice == '3':\n",
        "        print(\"Exiting program.\")\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "print(\"Program finished.\")\n",
        "Körültekintően használja a kódot\n",
        "Explanation of Changes:\n",
        "Main Program Loop: The code is now structured with a while True loop that presents the user with options (1, 2, or 3).\n",
        "User Input for Choice: Inside the loop, input(\"Enter your choice (1, 2, or 3): \") prompts the user to enter their selection.\n",
        "Conditional Execution:\n",
        "An if choice == '1': block handles the automated testing. It calls load_sentiment_model and classify_reviews with the predefined test_reviews.\n",
        "An elif choice == '2': block handles the user chat agent. It also loads the model and then enters a nested while True loop to interact with the user, calling classify_reviews for each input review.\n",
        "An elif choice == '3': block breaks out of the main loop, ending the program.\n",
        "An else: block handles invalid input.\n",
        "Model Loading within Choice Blocks: The load_sentiment_model function is called within both the automated testing and user chat blocks. This ensures that the model is loaded only when needed for the selected option.\n",
        "Now, when you run the code, you'll be prompted to choose which functionality you want to use before the automated testing or chat agent begins.\n",
        "—-----------------------------------------------------------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Py5qWQBBYu4I"
      }
    }
  ]
}